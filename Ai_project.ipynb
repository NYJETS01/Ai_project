{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b95a68",
   "metadata": {},
   "source": [
    "# Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db556c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b92065",
   "metadata": {},
   "source": [
    "## Load and concatenate multiple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "780fc5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(filenames):\n",
    "    dfs = []\n",
    "    for filename in filenames:\n",
    "        file_location = filename\n",
    "        df = pd.read_csv(file_location)\n",
    "        dfs.append(df)\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    return combined_df\n",
    "filenames = [\"cnbc_headlines.csv\", \"guardian_headlines.csv\", \"reuters_headlines.csv\"]\n",
    "# Load and preprocess datasets\n",
    "df = load_datasets(filenames)\n",
    "df = df.dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1106f6",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd09467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35515, 13070)\n",
      "(35515,)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the headlines using CountVectorizer with custom tokenizer\n",
    "def tokenize_stem(text):\n",
    "    blob = TextBlob(text)\n",
    "    return [word.stem() for word in blob.words]\n",
    "corpus = []\n",
    "for item in df['Headlines']:\n",
    "    corpus.append(' '.join(tokenize_stem(str(item))))\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = df.iloc[:, 0].values\n",
    "print(X.shape)\n",
    "print(y.shape)  # Print first few rows of the vectorized features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10abd2dd",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97f50ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment_score = blob.sentiment.polarity  # Get polarity score (-1 to 1)\n",
    "    if sentiment_score > 0:\n",
    "        return 'positive'\n",
    "    elif sentiment_score < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply sentiment analysis to 'Headlines' or 'Description' to derive sentiment labels\n",
    "df['Sentiment'] = df['Headlines'].apply(analyze_sentiment)\n",
    "\n",
    "# Split the data into training and testing sets based on derived sentiment labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['Headlines', 'Description']], df['Sentiment'], test_size=0.2, random_state=0)\n",
    "# Vectorize and concatenate training data\n",
    "cv = CountVectorizer()\n",
    "X_train_headlines = cv.fit_transform(X_train['Headlines']).toarray()\n",
    "X_train_description = cv.transform(X_train['Description']).toarray()\n",
    "X_train_processed = np.concatenate((X_train_headlines, X_train_description), axis=1)\n",
    "\n",
    "# Vectorize and concatenate testing data (using the same CountVectorizer)\n",
    "X_test_headlines = cv.transform(X_test['Headlines']).toarray()\n",
    "X_test_description = cv.transform(X_test['Description']).toarray()\n",
    "X_test_processed = np.concatenate((X_test_headlines, X_test_description), axis=1)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_train_onehot = to_categorical(y_train_encoded)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "y_test_onehot = to_categorical(y_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e2904",
   "metadata": {},
   "source": [
    "## Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8287cb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bmack\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train_processed.shape[1], activation='relu'),\n",
    "    Dropout(0.2),  \n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3), \n",
    "    Dense(3, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f6b81",
   "metadata": {},
   "source": [
    "## Compile and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41147e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 41ms/step - accuracy: 0.7084 - loss: 0.7418\n",
      "Epoch 2/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 42ms/step - accuracy: 0.9382 - loss: 0.1942\n",
      "Epoch 3/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 43ms/step - accuracy: 0.9820 - loss: 0.0605\n",
      "Epoch 4/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 43ms/step - accuracy: 0.9912 - loss: 0.0262\n",
      "Epoch 5/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 43ms/step - accuracy: 0.9938 - loss: 0.0207\n",
      "Epoch 6/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 44ms/step - accuracy: 0.9961 - loss: 0.0131\n",
      "Epoch 7/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 43ms/step - accuracy: 0.9969 - loss: 0.0086\n",
      "Epoch 8/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 40ms/step - accuracy: 0.9965 - loss: 0.0101\n",
      "Epoch 9/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 42ms/step - accuracy: 0.9976 - loss: 0.0080\n",
      "Epoch 10/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 42ms/step - accuracy: 0.9979 - loss: 0.0073\n",
      "Epoch 11/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 44ms/step - accuracy: 0.9982 - loss: 0.0049\n",
      "Epoch 12/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 42ms/step - accuracy: 0.9984 - loss: 0.0069\n",
      "Epoch 13/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 41ms/step - accuracy: 0.9993 - loss: 0.0035\n",
      "Epoch 14/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 43ms/step - accuracy: 0.9982 - loss: 0.0061\n",
      "Epoch 15/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 42ms/step - accuracy: 0.9986 - loss: 0.0043\n",
      "Epoch 16/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 42ms/step - accuracy: 0.9983 - loss: 0.0055\n",
      "Epoch 17/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 43ms/step - accuracy: 0.9987 - loss: 0.0034\n",
      "Epoch 18/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 42ms/step - accuracy: 0.9991 - loss: 0.0030\n",
      "Epoch 19/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 43ms/step - accuracy: 0.9992 - loss: 0.0023\n",
      "Epoch 20/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 42ms/step - accuracy: 0.9982 - loss: 0.0064\n",
      "Epoch 21/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 42ms/step - accuracy: 0.9998 - loss: 9.6529e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 42ms/step - accuracy: 0.9995 - loss: 0.0023\n",
      "Epoch 23/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 39ms/step - accuracy: 0.9991 - loss: 0.0030\n",
      "Epoch 24/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 41ms/step - accuracy: 0.9993 - loss: 0.0019\n",
      "Epoch 25/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 42ms/step - accuracy: 0.9996 - loss: 0.0017\n",
      "Epoch 26/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 41ms/step - accuracy: 0.9995 - loss: 0.0023\n",
      "Epoch 27/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 41ms/step - accuracy: 0.9993 - loss: 0.0026\n",
      "Epoch 28/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 41ms/step - accuracy: 0.9998 - loss: 8.6350e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 41ms/step - accuracy: 0.9996 - loss: 0.0015\n",
      "Epoch 30/30\n",
      "\u001b[1m888/888\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 41ms/step - accuracy: 0.9991 - loss: 0.0038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1dab49281d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_processed, y_train_onehot, epochs=30, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "040773fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n",
      "[2 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiment for test headlines using the trained neural network\n",
    "y_pred = model.predict(X_test_processed)\n",
    "y_pred = np.argmax(y_pred, axis=1)  # Convert softmax probabilities to class labels\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c756d8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.170756459236145, 0.859777569770813]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=X_test_processed, y=y_test_onehot, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f680707b",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using Vader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fefaeb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    scores = sid.polarity_scores(text)\n",
    "    compound_score = scores['compound']\n",
    "    if compound_score >= 0.05:\n",
    "        return 2  # Positive sentiment\n",
    "    elif compound_score <= 0.05:\n",
    "        return 0  # Negative sentiment\n",
    "    else:\n",
    "        return 1  # Neutral sentiment\n",
    "\n",
    "# Predict sentiment for test headlines using VADER\n",
    "y_pred_vader = [analyze_sentiment(headline) for headline in df.loc[X_test.index]['Headlines']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800f8cdd",
   "metadata": {},
   "source": [
    "## Comparing Accuracy Amongst the two techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d12dc065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Model Accuracy: 0.8597775587779811\n",
      "VADER Sentiment Analysis Accuracy: 0.21469801492327187\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy_nn = np.sum(y_pred == y_test_encoded) / len(y_test_encoded)\n",
    "print(\"Neural Network Model Accuracy:\", accuracy_nn)\n",
    "\n",
    "# Convert VADER predictions to array\n",
    "y_pred_vader = np.array(y_pred_vader)\n",
    "\n",
    "# Calculate accuracy for VADER sentiment analysis\n",
    "accuracy_vader = np.sum(y_pred_vader == y_test_encoded) / len(y_test_encoded)\n",
    "print(\"VADER Sentiment Analysis Accuracy:\", accuracy_vader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
